{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidarvai/MRI-Image-Viewer/blob/main/UnetKod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMp2e897Nvt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0136998a-7156-48f5-9f98-d50bdffbba04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4rMHbdKNi3U",
        "outputId": "c07d5d2a-ae9e-426d-e44d-fca5f304994c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:46\u001b[0m 32s/step - accuracy: 0.1734 - loss: 0.6932\n",
            "Epoch 1: loss improved from inf to 0.69318, saving model to best_unet_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - accuracy: 0.1734 - loss: 0.6932 - learning_rate: 0.0010\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:23\u001b[0m 23s/step - accuracy: 0.2408 - loss: 0.6932\n",
            "Epoch 2: loss improved from 0.69318 to 0.69316, saving model to best_unet_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 17ms/step - accuracy: 0.2408 - loss: 0.6932 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:41\u001b[0m 25s/step - accuracy: 0.1585 - loss: 0.6932\n",
            "Epoch 3: loss improved from 0.69316 to 0.69315, saving model to best_unet_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 16ms/step - accuracy: 0.1585 - loss: 0.6932 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:07\u001b[0m 41s/step - accuracy: 0.3813 - loss: 0.6931\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 4: loss improved from 0.69315 to 0.69315, saving model to best_unet_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.3813 - loss: 0.6931 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:20\u001b[0m 22s/step - accuracy: 0.4618 - loss: 0.6931\n",
            "Epoch 5: loss did not improve from 0.69315\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 2ms/step - accuracy: 0.4618 - loss: 0.6931 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:14\u001b[0m 42s/step - accuracy: 0.4568 - loss: 0.6931\n",
            "Epoch 6: loss did not improve from 0.69315\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - accuracy: 0.4568 - loss: 0.6931 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 24s/step - accuracy: 0.4077 - loss: 0.6931\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 7: loss did not improve from 0.69315\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 2ms/step - accuracy: 0.4077 - loss: 0.6931 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 23s/step - accuracy: 0.3133 - loss: 0.6931\n",
            "Epoch 8: loss improved from 0.69315 to 0.69315, saving model to best_unet_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 17ms/step - accuracy: 0.3133 - loss: 0.6931 - learning_rate: 2.5000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:18\u001b[0m 42s/step - accuracy: 0.2942 - loss: 0.6931\n",
            "Epoch 9: loss did not improve from 0.69315\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 3ms/step - accuracy: 0.2942 - loss: 0.6931 - learning_rate: 2.5000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:23\u001b[0m 23s/step - accuracy: 0.2995 - loss: 0.6931\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 10: loss did not improve from 0.69315\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2ms/step - accuracy: 0.2995 - loss: 0.6931 - learning_rate: 2.5000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:41\u001b[0m 25s/step - accuracy: 0.3204 - loss: 0.6931\n",
            "Epoch 11: loss did not improve from 0.69315\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.3204 - loss: 0.6931 - learning_rate: 1.2500e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:20\u001b[0m 22s/step - accuracy: 0.3397 - loss: 0.6931\n",
            "Epoch 12: loss improved from 0.69315 to 0.69314, saving model to best_unet_model.keras\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 18ms/step - accuracy: 0.3397 - loss: 0.6931 - learning_rate: 1.2500e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:07\u001b[0m 41s/step - accuracy: 0.3604 - loss: 0.6931\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\n",
            "Epoch 13: loss did not improve from 0.69314\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - accuracy: 0.3604 - loss: 0.6931 - learning_rate: 1.2500e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:08\u001b[0m 41s/step - accuracy: 0.3789 - loss: 0.6931\n",
            "Epoch 14: loss did not improve from 0.69314\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - accuracy: 0.3789 - loss: 0.6931 - learning_rate: 6.2500e-05\n",
            "Epoch 15/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:08\u001b[0m 41s/step - accuracy: 0.3839 - loss: 0.6931\n",
            "Epoch 15: loss did not improve from 0.69314\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1ms/step - accuracy: 0.3839 - loss: 0.6931 - learning_rate: 6.2500e-05\n",
            "Epoch 16/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:33\u001b[0m 24s/step - accuracy: 0.3907 - loss: 0.6931\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "\n",
            "Epoch 16: loss did not improve from 0.69314\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3ms/step - accuracy: 0.3907 - loss: 0.6931 - learning_rate: 6.2500e-05\n",
            "Epoch 17/50\n",
            "\u001b[1m 1/10\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:56\u001b[0m 40s/step - accuracy: 0.3932 - loss: 0.6931\n",
            "Epoch 17: loss did not improve from 0.69314\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step - accuracy: 0.3932 - loss: 0.6931 - learning_rate: 3.1250e-05\n",
            "Epoch 17: early stopping\n",
            "Restoring model weights from the end of the best epoch: 12.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import csv\n",
        "\n",
        "# Paths\n",
        "pathSection = '/content/drive/My Drive/Allamvizsga/MRI_felvetel/Teszt'\n",
        "pathSection1To7 = '/content/drive/My Drive/Allamvizsga/Teszt_folder/hg000.csv'\n",
        "output_text_file = '/content/drive/My Drive/Allamvizsga/Eredmeny/output_metrics_unet.txt'\n",
        "output_csv_file = '/content/drive/My Drive/Allamvizsga/Eredmeny/output_unet.csv'\n",
        "\n",
        "# CSV Header\n",
        "header = ['volumeName', 'tumorType', 'truePositive', 'trueNegative', 'falsePositive', 'falseNegative',\n",
        "          'truePositiveRate', 'trueNegativeRate', 'positivePredictiveValue',\n",
        "          'negativePredictiveValue', 'accuracy', 'diceScore']\n",
        "\n",
        "# Save header to CSV\n",
        "with open(output_csv_file, 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(header)\n",
        "\n",
        "# Define U-Net Model\n",
        "def unet_model(input_size=(256, 256, 1)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "\n",
        "    up1 = UpSampling2D(size=(2, 2))(conv3)\n",
        "    up1 = Concatenate()([up1, conv2])\n",
        "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(up1)\n",
        "    conv4 = Conv2D(128, 3, activation='relu', padding='same')(conv4)\n",
        "\n",
        "    up2 = UpSampling2D(size=(2, 2))(conv4)\n",
        "    up2 = Concatenate()([up2, conv1])\n",
        "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(up2)\n",
        "    conv5 = Conv2D(64, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    outputs = Conv2D(4, 1, activation='sigmoid')(conv5)  # 4 output channels for each tumor type\n",
        "    model = Model(inputs=[inputs], outputs=[outputs])\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Data Loader Placeholder\n",
        "def load_image_data_for_each_file(file_path):\n",
        "    # Replace this placeholder with actual image loading logic\n",
        "    X = np.random.rand(5, 256, 256, 1)  # Example batch of 5 images\n",
        "    Y = np.random.randint(0, 2, (5, 256, 256, 4))  # Example batch of 4-channel masks\n",
        "    return X, Y\n",
        "\n",
        "# Metrics Calculation\n",
        "def calculate_metrics(y_true, y_pred, tumorType, volumeName):\n",
        "    y_true = y_true.flatten()\n",
        "    y_pred = (y_pred.flatten() > 0.5).astype(int)\n",
        "\n",
        "    CM = confusion_matrix(y_true, y_pred)\n",
        "    TN, FP, FN, TP = CM.ravel() if CM.size == 4 else (0, 0, 0, 0)\n",
        "\n",
        "    TPR = round(TP / (TP + FN), 3) if TP + FN > 0 else 0\n",
        "    TNR = round(TN / (TN + FP), 3) if TN + FP > 0 else 0\n",
        "    PPV = round(TP / (TP + FP), 3) if TP + FP > 0 else 0\n",
        "    NPV = round(TN / (TN + FN), 3) if TN + FN > 0 else 0\n",
        "    ACC = round((TP + TN) / (TP + FP + FN + TN), 3)\n",
        "    DS = round((2 * TP) / ((2 * TP) + FP + FN), 3) if (2 * TP) + FP + FN > 0 else 0\n",
        "\n",
        "    # Save metrics to CSV\n",
        "    with open(output_csv_file, 'a+', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([volumeName, tumorType, TP, TN, FP, FN, TPR, TNR, PPV, NPV, ACC, DS])\n",
        "\n",
        "    # Save metrics to text file\n",
        "    with open(output_text_file, 'a') as txt_file:\n",
        "        txt_file.write(f\"\\n=== {tumorType} Metrics for file: {volumeName} ===\\n\")\n",
        "        txt_file.write(f\"Confusion Matrix:\\n{CM}\\n\")\n",
        "        txt_file.write(f\"True positive (TP): {TP}\\n\")\n",
        "        txt_file.write(f\"True negative (TN): {TN}\\n\")\n",
        "        txt_file.write(f\"False positive (FP): {FP}\\n\")\n",
        "        txt_file.write(f\"False negative (FN): {FN}\\n\")\n",
        "        txt_file.write(f\"True positive rate (TPR): {TPR:.3f}\\n\")\n",
        "        txt_file.write(f\"True negative rate (TNR): {TNR:.3f}\\n\")\n",
        "        txt_file.write(f\"Positive predictive value (PPV): {PPV:.3f}\\n\")\n",
        "        txt_file.write(f\"Negative predictive value (NPV): {NPV:.3f}\\n\")\n",
        "        txt_file.write(f\"Accuracy (ACC): {ACC:.3f}\\n\")\n",
        "        txt_file.write(f\"Dice score (DS): {DS:.3f}\\n\\n\")\n",
        "\n",
        "# Load Data\n",
        "X_sample, Y_sample = load_image_data_for_each_file(pathSection1To7)\n",
        "\n",
        "# Initialize Model\n",
        "unet = unet_model()\n",
        "\n",
        "# Augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "augmented_data = datagen.flow(X_sample, Y_sample, batch_size=8)\n",
        "\n",
        "# Training Callbacks\n",
        "callbacks = [\n",
        "    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=1),\n",
        "    EarlyStopping(monitor='loss', patience=5, verbose=1, restore_best_weights=True),\n",
        "    ModelCheckpoint('best_unet_model.keras', save_best_only=True, monitor='loss', verbose=1)  # Updated file extension\n",
        "]\n",
        "\n",
        "# Train Model\n",
        "history = unet.fit(\n",
        "    augmented_data,\n",
        "    epochs=50,\n",
        "    steps_per_epoch=10,  # Adjust as per dataset size\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Process Each Test File\n",
        "tumor_types = [\"Whole Tumor\", \"Edema\", \"Tumor Core\", \"Enhancing Core\"]\n",
        "\n",
        "for filename in os.listdir(pathSection):\n",
        "    file_path = os.path.join(pathSection, filename)\n",
        "    if os.path.isfile(file_path):\n",
        "        # Load data\n",
        "        X, Y = load_image_data_for_each_file(file_path)\n",
        "        y_pred = unet.predict(X)\n",
        "\n",
        "        volumeName = os.path.basename(file_path)\n",
        "\n",
        "        for idx, tumorType in enumerate(tumor_types):\n",
        "            calculate_metrics(Y[..., idx], y_pred[..., idx], tumorType, volumeName)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzGq5Tgwwl9RgJmSvHo5Gk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}